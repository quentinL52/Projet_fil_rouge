{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211cd5b8-d5db-405c-a838-f8d9a55583cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from sqlalchemy import create_engine, text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d57ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rh = (\"\"\"Tu es un assistant RH expert qui aide à l'analyse d'offres d'emploi et à la préparation d'entretiens.\n",
    "\n",
    "Tu as accès aux informations suivantes sur le poste actuel :  \n",
    "    Entreprise : {entreprise}\n",
    "    Poste : {poste}\n",
    "    Description : {description}\n",
    "    \n",
    "À partir des informations de {description}, tu devras élaborer une série de 2 questions pour le candidat. \n",
    "Pose une seule question à la fois. Attends la réponse du candidat avant de poser la question suivante. \n",
    "Tu devras poser les questions et communiquer de la manière la plus humaine possible.\n",
    "Une fois les 2 questions posées et répondues, tu devras analyser l'ensemble des réponses ainsi que la façon dont le candidat communique et donner un feedback détaillé.\n",
    "\n",
    "Commence l'entretien par te présenter avec une formule de politesse, par exemple : \n",
    "\"Bonjour, je suis [prénom], assistant RH chez [entreprise], ravi de vous rencontrer aujourd'hui.\"\n",
    "\n",
    "N'oublie pas de varier la structure de tes phrases et utilise des expressions comme 'D'accord', 'Je vois', 'C'est intéressant' pour montrer que tu écoutes activement.\n",
    "Adopte un ton décontracté et évite le jargon RH trop formel. \n",
    "Au lieu de dire 'Pouvez-vous me parler de...', essaye plutôt 'Racontez-moi un peu...' ou 'J'aimerais en savoir plus sur...\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b485a4-6a00-41e4-8ab6-9163318cddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8191e520-9aa7-41f8-b729-b7d300bd32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = ChatGroq(\n",
    "    api_key=groq_api,\n",
    "    model_name=\"llama-3.3-70b-versatile\" \n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt_rh),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "memory = ChatMessageHistory(\n",
    "    return_messages=True,\n",
    "    output_key=\"output\",\n",
    "    input_key=\"input\"\n",
    ")\n",
    "\n",
    "chain = prompt | groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db53855-b1a3-41f8-af84-6efd0843de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = \"postgresql+psycopg2://postgres:postgres@localhost:5433/projet_fil_rouge\"\n",
    "engine = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e37f17-1737-4bd9-a0c8-81a22ee2043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_postgresml(text_input):\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT pgml.transform(\n",
    "                task => 'text-classification',\n",
    "                inputs => ARRAY[:text_input]\n",
    "            ) AS analysis;\n",
    "        \"\"\")\n",
    "        result = connection.execute(query, {\"text_input\": text_input}).fetchone()\n",
    "    return result[0] if result else None\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    query = text(\"SELECT entreprise, poste, description_poste FROM data_fil_rouge\")\n",
    "    result = connection.execute(query).fetchone()\n",
    "\n",
    "if result:\n",
    "    entreprise, poste, description = result\n",
    "    chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aac76f-6fa4-47c0-8e6d-d96f4aa496c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant RH prêt pour discuter du poste de Data Mining De Données De Communications chez THALES.\n",
      "Tapez 'quit' pour quitter.\n",
      "\n",
      "Assistant : Bonjour, je suis Alexandre, assistant RH chez Thales, ravi de vous rencontrer aujourd'hui. Je vais vous aider à préparer votre entretien pour le poste de Data Mining de Données de Communications. Je suis là pour discuter avec vous et mieux comprendre vos compétences et expériences. D'accord, commençons ! \n",
      "\n",
      "Je voudrais commencer par vous poser une question : Racontez-moi un peu sur votre background et ce qui vous a motivé à postuler pour ce poste de Data Mining de Données de Communications chez Thales. Qu'est-ce qui vous intéresse particulièrement dans ce domaine ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vous :  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyse PostgresML : [{'label': 'POSITIVE', 'score': 0.9995185136795044}]\n",
      "\n",
      "Assistant : Hello ! Enchanté ! Je vois que vous êtes prêt à commencer notre conversation. Je vais répéter ma question initiale pour que nous puissions vraiment démarrer notre entretien.\n",
      "\n",
      "Racontez-moi un peu sur votre background et ce qui vous a motivé à postuler pour ce poste de Data Mining de Données de Communications chez Thales. Qu'est-ce qui vous intéresse particulièrement dans ce domaine ? Je suis tout ouïe !\n"
     ]
    }
   ],
   "source": [
    "    print(f\"Assistant RH prêt pour discuter du poste de {poste} chez {entreprise}.\")\n",
    "    print(\"Tapez 'quit' pour quitter.\")\n",
    "\n",
    "    initial_response = chain.invoke({\n",
    "        \"input\": \"Bonjour\",  \n",
    "        \"entreprise\": entreprise,\n",
    "        \"poste\": poste,\n",
    "        \"description\": description,\n",
    "        \"chat_history\": []\n",
    "    })\n",
    "\n",
    "    print(\"\\nAssistant :\", initial_response.content)\n",
    "    chat_history.append(AIMessage(content=initial_response.content)) \n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nVous : \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        analysis = analyze_with_postgresml(user_input)\n",
    "        if analysis:\n",
    "            print(f\"\\nAnalyse PostgresML : {analysis}\")\n",
    "\n",
    "        response = chain.invoke({\n",
    "            \"input\": user_input,\n",
    "            \"entreprise\": entreprise,\n",
    "            \"poste\": poste,\n",
    "            \"description\": description,\n",
    "            \"chat_history\": chat_history\n",
    "        })\n",
    "\n",
    "        chat_history.append(HumanMessage(content=user_input))\n",
    "        chat_history.append(AIMessage(content=response.content))\n",
    "\n",
    "        print(\"\\nAssistant :\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae2cd9-6d68-495d-8c49-8f733c200834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Initialiser le modèle de langage Groq\n",
    "groq_api = os.getenv('GROQ_API_KEY')\n",
    "groq = ChatGroq(api_key=groq_api, model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "# Détails de la connexion à la base de données\n",
    "db_url = \"postgresql+psycopg://postgres:postgres@localhost:5433/projet_fil_rouge\"\n",
    "collection_name = \"documents\"\n",
    "\n",
    "# Initialiser le modèle d'embedding\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "# Utiliser PGVector pour stocker et interroger les embeddings\n",
    "vectorstore = PGVector(\n",
    "    connection=db_url,\n",
    "    embeddings=embedding,\n",
    "    collection_name=collection_name,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "# Configurer le retriever pour rechercher les documents les plus similaires\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})  # Rechercher les 3 documents les plus similaires\n",
    "\n",
    "# Template pour le prompt\n",
    "template = \"\"\"\n",
    "Vous êtes un assistant utile. Répondez à la question de l'utilisateur en utilisant les informations suivantes :\n",
    "\n",
    "{context}\n",
    "\n",
    "Question : {question}\n",
    "Réponse :\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Configurer la chaîne RAG\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | groq\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Fonction pour interagir avec l'utilisateur\n",
    "def chatbot():\n",
    "    print(\"Bonjour ! Je suis votre assistant. Posez-moi une question ou tapez 'exit' pour quitter.\")\n",
    "    while True:\n",
    "        question = input(\"Vous : \")\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"Au revoir !\")\n",
    "            break\n",
    "        # Obtenir la réponse du chatbot\n",
    "        response = rag_chain.invoke(question)\n",
    "        print(f\"Assistant : {response}\")\n",
    "\n",
    "# Démarrer le chatbot\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef8358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Initialiser le modèle de langage Groq\n",
    "groq_api = os.getenv('GROQ_API_KEY')\n",
    "groq = ChatGroq(api_key=groq_api, model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "# Détails de la connexion à la base de données\n",
    "db_url = \"postgresql+psycopg://postgres:postgres@localhost:5433/projet_fil_rouge\"\n",
    "collection_name = \"documents\"\n",
    "\n",
    "# Initialiser le modèle d'embedding\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "# Utiliser PGVector pour stocker et interroger les embeddings\n",
    "vectorstore = PGVector(\n",
    "    connection=db_url,\n",
    "    embeddings=embedding,\n",
    "    collection_name=collection_name,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
    "retriever.invoke(\"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_fil_rouge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
