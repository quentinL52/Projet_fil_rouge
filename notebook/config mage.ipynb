{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/uVwOMthHutQimThAdDNw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# parametre du pilote et librairie du container docker"],"metadata":{"id":"cb9lP25a2doe"}},{"cell_type":"markdown","source":["fonctionne mais il faut le parametrer dans le container docker\n","en ajoutant dans le docker file les parametres suivant :"],"metadata":{"id":"w3CuaEPf2se9"}},{"cell_type":"code","source":["# Installation des dépendances pour Chrome\n","RUN apt-get update && apt-get install -y \\\n","    wget \\\n","    curl \\\n","    unzip \\\n","    gnupg2 \\\n","    xvfb \\\n","    libxi6 \\\n","    libgconf-2-4 \\\n","    libnss3 \\\n","    libgbm1 \\\n","    libasound2 \\\n","    libxss1 \\\n","    fonts-liberation \\\n","    libappindicator3-1 \\\n","    libatk-bridge2.0-0 \\\n","    libatspi2.0-0 \\\n","    libgtk-3-0 \\\n","    libx11-xcb1 \\\n","    xdg-utils \\\n","    && rm -rf /var/lib/apt/lists/*\n","\n","# Installation de Chrome\n","RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \\\n","    && echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google-chrome.list \\\n","    && apt-get update \\\n","    && apt-get install -y google-chrome-stable \\\n","    && rm -rf /var/lib/apt/lists/*\n","\n","# Installation de ChromeDriver correspondant à Chrome 130\n","RUN wget -q \"https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/130.0.6723.69/linux64/chromedriver-linux64.zip\" \\\n","    && unzip chromedriver-linux64.zip \\\n","    && mv chromedriver-linux64/chromedriver /usr/local/bin/ \\\n","    && rm -rf chromedriver-linux64.zip chromedriver-linux64 \\\n","    && chmod +x /usr/local/bin/chromedriver\n","\n","# Configuration des permissions\n","RUN chmod -R 777 /root/.cache"],"metadata":{"id":"-z9fi7b42omN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# cadremploi (En chantier)"],"metadata":{"id":"QmhyERVN_UsJ"}},{"cell_type":"markdown","source":["a retravailler avec selenium en utilisant la meme logique que pour welcome to the jungle"],"metadata":{"id":"DdhZl_qU_dLD"}},{"cell_type":"code","source":["//*[@id=\"liste-postuler\"]/div[1]/li/a[1]\n","//*[@id=\"liste-postuler\"]/div[1]/li/a[1]\n","//*[@id=\"liste-postuler\"]/div[3]/li/a[1]\n","//*[@id=\"liste-postuler\"]/div[6]/li/a[1]\n","//*[@id=\"liste-postuler\"]/div[8]/li/a[1]\n","\n","//*[@id=\"liste-postuler\"]/b/div[22]/li/a[1]\n","//*[@id=\"liste-postuler\"]/b/div[26]/li/a[1]\n","\n","# pour recuper l'url d'une offre specifique :\n","<a class=\"row js-lien-detail-offre offre\" href=\"/emploi/detail_offre?offreId=156453275908950362\"\n","\n","# pour la date de publication :\n","<time class=\"date-publication\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"TAgTVPqyCft8","executionInfo":{"status":"error","timestamp":1730034398745,"user_tz":-60,"elapsed":401,"user":{"displayName":"Quentin Loumeau","userId":"17528108048197558206"}},"outputId":"56b40528-f1ce-4de0-82a0-fe4cf7564f40"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-2-2e5d9cadf535>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-2e5d9cadf535>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    //*[@id=\"liste-postuler\"]/div[1]/li/a[1]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gja6UWEf9JX7"},"outputs":[],"source":["if 'data_loader':\n","    from selenium import webdriver\n","    from selenium.webdriver.common.by import By\n","    from selenium.webdriver.support.ui import WebDriverWait\n","    from selenium.webdriver.support import expected_conditions as EC\n","    from selenium.webdriver.chrome.service import Service\n","    import pandas as pd\n","    import time\n","    import re\n","\n","    @data_loader\n","    def load_data(*args, **kwargs):\n","        \"\"\"\n","        Fonction de chargement des données pour Mage\n","        \"\"\"\n","        def parse_time_ago(time_text):\n","            if 'heures' in time_text or 'heure' in time_text:\n","                hours = int(re.search(r'\\d+', time_text).group())\n","                return hours\n","            elif 'minutes' in time_text:\n","                minutes = int(re.search(r'\\d+', time_text).group())\n","                return minutes / 60\n","            else:\n","                return 24\n","\n","        options = webdriver.ChromeOptions()\n","        options.add_argument('--headless')\n","        options.add_argument('--disable-gpu')\n","        options.add_argument('--no-sandbox')\n","        options.add_argument('--disable-dev-shm-usage')\n","        options.binary_location = '/usr/bin/google-chrome'\n","\n","        try:\n","            service = Service(executable_path='/usr/local/bin/chromedriver')\n","            driver = webdriver.Chrome(service=service, options=options)\n","\n","            url = \"https://www.cadremploi.fr/emploi/liste_offres?locLibre=ile%20de%20france&motscles=data\"\n","\n","            driver.get(url)\n","\n","            wait = WebDriverWait(driver, 10)\n","            base_xpath = '//*[@id=\"app\"]/div/div/div/div[2]/div/ul/li'\n","            wait.until(EC.presence_of_element_located((By.XPATH, base_xpath)))\n","\n","            jobs_data = []\n","            should_continue_scrolling = True\n","\n","            while should_continue_scrolling:\n","                visible_elements = []\n","                i = 1\n","                while True:\n","                    try:\n","                        element_xpath = f'//*[@id=\"app\"]/div/div/div/div[2]/div/ul/li[{i}]'\n","                        element = driver.find_element(By.XPATH, element_xpath)\n","                        visible_elements.append(element)\n","                        i += 1\n","                    except:\n","                        break\n","\n","                for element in visible_elements:\n","                    try:\n","                        url_element = element.find_element(By.XPATH, './/div/div/div/div[2]/a')\n","                        url = url_element.get_attribute('href')\n","\n","                        time_element = element.find_element(By.XPATH, './/time/span')\n","                        time_ago = time_element.text\n","\n","                        hours_ago = parse_time_ago(time_ago)\n","\n","                        if hours_ago < 24:\n","                            jobs_data.append({\n","                                'url': url,\n","                                'published': time_ago,\n","                                'hours_ago': hours_ago\n","                            })\n","                        else:\n","                            should_continue_scrolling = False\n","                            break\n","\n","                    except Exception as e:\n","                        continue\n","\n","                if should_continue_scrolling:\n","                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n","                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","                    time.sleep(2)\n","                    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n","\n","                    if new_height == last_height:\n","                        break\n","\n","            df = pd.DataFrame(jobs_data)\n","            return df\n","\n","        except Exception as e:\n","            import traceback\n","            print(traceback.format_exc())\n","            return pd.DataFrame()\n","\n","        finally:\n","            if 'driver' in locals():\n","                driver.quit()"]},{"cell_type":"markdown","source":["# WTTJ"],"metadata":{"id":"vbyopaNzIk4F"}},{"cell_type":"markdown","source":["\n","\n","*   recuperation des URL d'offre de la premiére page :\n","\n"],"metadata":{"id":"pOhJ_pEHJ6L9"}},{"cell_type":"code","source":["if 'data_loader':\n","    from selenium import webdriver\n","    from selenium.webdriver.common.by import By\n","    from selenium.webdriver.support.ui import WebDriverWait\n","    from selenium.webdriver.support import expected_conditions as EC\n","    from selenium.webdriver.chrome.service import Service\n","    import pandas as pd\n","    import time\n","    import re\n","\n","    @data_loader\n","    def load_data(*args, **kwargs):\n","        \"\"\"\n","        Fonction de chargement des données pour Mage\n","        \"\"\"\n","        def parse_time_ago(time_text):\n","            if 'heures' in time_text or 'heure' in time_text:\n","                hours = int(re.search(r'\\d+', time_text).group())\n","                return hours\n","            elif 'minutes' in time_text:\n","                minutes = int(re.search(r'\\d+', time_text).group())\n","                return minutes / 60\n","            else:\n","                return 24\n","\n","        options = webdriver.ChromeOptions()\n","        options.add_argument('--headless')\n","        options.add_argument('--disable-gpu')\n","        options.add_argument('--no-sandbox')\n","        options.add_argument('--disable-dev-shm-usage')\n","        options.binary_location = '/usr/bin/google-chrome'\n","\n","        try:\n","            service = Service(executable_path='/usr/local/bin/chromedriver')\n","            driver = webdriver.Chrome(service=service, options=options)\n","\n","            url = \"https://www.welcometothejungle.com/fr/jobs?query=data&refinementList%5Bcontract_type%5D%5B%5D=internship&refinementList%5Bcontract_type%5D%5B%5D=temporary&refinementList%5Bcontract_type%5D%5B%5D=apprenticeship&refinementList%5Boffices.state%5D%5B%5D=%C3%8Ele-de-France&refinementList%5Boffices.country_code%5D%5B%5D=FR&page=1&aroundQuery=%C3%8Ele-de-France%2C%20France&sortBy=mostRelevant\"\n","\n","            driver.get(url)\n","\n","            wait = WebDriverWait(driver, 10)\n","            base_xpath = '//*[@id=\"app\"]/div/div/div/div[2]/div/ul/li'\n","            wait.until(EC.presence_of_element_located((By.XPATH, base_xpath)))\n","\n","            jobs_data = []\n","            should_continue_scrolling = True\n","\n","            while should_continue_scrolling:\n","                visible_elements = []\n","                i = 1\n","                while True:\n","                    try:\n","                        element_xpath = f'//*[@id=\"app\"]/div/div/div/div[2]/div/ul/li[{i}]'\n","                        element = driver.find_element(By.XPATH, element_xpath)\n","                        visible_elements.append(element)\n","                        i += 1\n","                    except:\n","                        break\n","\n","                for element in visible_elements:\n","                    try:\n","                        url_element = element.find_element(By.XPATH, './/div/div/div/div[2]/a')\n","                        url = url_element.get_attribute('href')\n","\n","                        time_element = element.find_element(By.XPATH, './/time/span')\n","                        time_ago = time_element.text\n","\n","                        hours_ago = parse_time_ago(time_ago)\n","\n","                        if hours_ago < 24:\n","                            jobs_data.append({\n","                                'url': url,\n","                                'published': time_ago,\n","                                'hours_ago': hours_ago\n","                            })\n","                        else:\n","                            should_continue_scrolling = False\n","                            break\n","\n","                    except Exception as e:\n","                        continue\n","\n","                if should_continue_scrolling:\n","                    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n","                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n","                    time.sleep(2)\n","                    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n","\n","                    if new_height == last_height:\n","                        break\n","\n","            df = pd.DataFrame(jobs_data)\n","            return df\n","\n","        except Exception as e:\n","            import traceback\n","            print(traceback.format_exc())\n","            return pd.DataFrame()\n","\n","        finally:\n","            if 'driver' in locals():\n","                driver.quit()"],"metadata":{"id":"MEyQI4_cJzDV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","*   transformer pour recuperer les elements que je veut sur chaque url\n","\n"],"metadata":{"id":"mn1woPiWKHra"}},{"cell_type":"code","source":["if 'transformer':\n","    from selenium import webdriver\n","    from selenium.webdriver.common.by import By\n","    from selenium.webdriver.support.ui import WebDriverWait\n","    from selenium.webdriver.support import expected_conditions as EC\n","    from selenium.webdriver.chrome.service import Service\n","    from selenium.webdriver.chrome.options import Options\n","    from webdriver_manager.chrome import ChromeDriverManager\n","    import pandas as pd\n","    import time\n","    from datetime import datetime\n","    import pytz\n","\n","    @transformer\n","    def transform_df(df, *args, **kwargs):\n","        \"\"\"\n","        Extrait les informations détaillées des offres d'emploi\n","        \"\"\"\n","        if df is None or df.empty:\n","            print(\"Aucune URL à traiter\")\n","            return pd.DataFrame()\n","\n","        options = Options()\n","        options.add_argument('--headless')\n","        options.add_argument('--no-sandbox')\n","        options.add_argument('--disable-dev-shm-usage')\n","        options.add_argument('--disable-gpu')\n","        options.add_argument('--window-size=1920,1080')\n","\n","        paris_tz = pytz.timezone('Europe/Paris')\n","        current_date = datetime.now(paris_tz).strftime(\"%d-%m-%Y\")\n","\n","        data = {\n","            'entreprise': [],\n","            'publication': [],\n","            'poste': [],\n","            'contrat': [],\n","            'profil': [],\n","            'description': [],\n","            'ville': [],\n","            'lien': [],\n","            'source': []\n","        }\n","\n","        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n","        wait = WebDriverWait(driver, 10)\n","\n","        try:\n","            total_urls = len(df)\n","\n","            for index, row in df.iterrows():\n","                url = row['url']\n","\n","                try:\n","                    driver.get(url)\n","                    time.sleep(2)\n","\n","                    def safe_extract(xpath, attribute=None):\n","                        try:\n","                            element = wait.until(EC.presence_of_element_located((By.XPATH, xpath)))\n","                            return element.get_attribute(attribute) if attribute else element.text\n","                        except Exception:\n","                            return \"Non disponible\"\n","\n","                    data['entreprise'].append(safe_extract('//*[@id=\"app\"]/div/div/div/div/div[3]/section/div[1]/div[1]/a/div/span'))\n","                    data['publication'].append(current_date)\n","                    data['poste'].append(safe_extract('//*[@id=\"app\"]/div/div/div/div/div[3]/section/div[1]/h2'))\n","                    data['contrat'].append(safe_extract('//*[@id=\"app\"]/div/div/div/div/div[3]/section/div[1]/div[2]/div/div/div[1]'))\n","                    data['profil'].append(safe_extract('//*[@id=\"the-position-section\"]/div/div[2]/div[2]/div/div[1]'))\n","                    data['description'].append(safe_extract('//*[@id=\"the-position-section\"]/div/div[2]/div[1]/div/div[1]'))\n","                    data['ville'].append(safe_extract('//*[@id=\"app\"]/div/div/div/div/div[3]/section/div[1]/div[2]/div/div/div[2]/span/span'))\n","                    data['lien'].append(url)\n","                    data['source'].append(\"Welcome to the Jungle\")\n","\n","                except Exception as e:\n","                    continue\n","\n","                time.sleep(1)\n","\n","            extracted_df = pd.DataFrame(data)\n","            return extracted_df\n","\n","        except Exception as e:\n","            return pd.DataFrame()\n","\n","        finally:\n","            driver.quit()"],"metadata":{"id":"oRcni1xtKnTv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","if 'transformer' not in globals():\n","    from mage_ai.data_preparation.decorators import transformer\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@transformer\n","def clean_dataframe(df, *args, **kwargs):\n","    \"\"\"\n","    Nettoie le DataFrame\n","    \"\"\"\n","    df_clean = df.copy()\n","    df_clean['publication'] = pd.to_datetime(df_clean['publication'], dayfirst=True, errors='coerce')\n","    df_clean['publication'] = df_clean['publication'].dt.strftime('%d-%m-%Y')\n","    df_clean = df_clean.drop_duplicates()\n","    return df_clean\n","\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Tests de validation\n","    \"\"\"\n","    assert output is not None, 'La sortie est non définie'\n","    assert len(output) > 0, 'Le DataFrame est vide'\n","    date_format = output['publication'].str.match(r'\\d{2}-\\d{2}-\\d{4}')\n","    assert date_format.all(), 'Format de date incorrect'\n","    assert len(output) == len(output.drop_duplicates()), 'Il reste des doublons dans le DataFrame'"],"metadata":{"id":"7Ja73oZi97Vb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","*   data exporter pour pouvoir stocker temporairement les données sur mon bucket minio  \n","  \n","le script va charger le fichier csv deja existant depuis le bucket, verifier les doublons avec les nouvelles données, puis concat les nouvelles offres dans le csv, pour le stocker"],"metadata":{"id":"sonrDJwzKqgz"}},{"cell_type":"code","source":["import pandas as pd\n","from io import BytesIO\n","from minio import Minio\n","import os\n","from datetime import datetime\n","import pytz\n","\n","@data_exporter\n","def export_to_minio(df, *args, **kwargs):\n","    \"\"\"\n","    Exporte le DataFrame des offres vers MinIO en écrasant l'ancien fichier\n","    \"\"\"\n","    client = Minio(\n","        \"minio:9000\",\n","        access_key=os.environ.get('MINIO_ACCESS_KEY'),\n","        secret_key=os.environ.get('MINIO_SECRET_KEY'),\n","        secure=False\n","    )\n","\n","    bucket_name = \"offresdata\"\n","    file_name = \"welcometothejungle.csv\"\n","\n","    if not client.bucket_exists(bucket_name):\n","        client.make_bucket(bucket_name)\n","    df = df.drop_duplicates(subset=['lien'], keep='last')\n","    csv_bytes = df.to_csv(index=False).encode('utf-8')\n","    client.put_object(\n","        bucket_name,\n","        file_name,\n","        BytesIO(csv_bytes),\n","        length=len(csv_bytes),\n","        content_type='application/csv'\n","    )\n","\n","    return f\"✅ {len(df)} offres exportées\"\n","\n","@test\n","def test_output(output, *args) -> None:\n","    assert output is not None and isinstance(output, str), 'La sortie doit être une chaîne non vide'"],"metadata":{"id":"lU4dvxjzK2UX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# hellowork"],"metadata":{"id":"lsX0bEEzMFev"}},{"cell_type":"markdown","source":["va simplement recuperer les données de l'api hellowork  \n","en divisant le loader en trois :\n","\n","*   CDD\n","\n","*   Stage\n","*   Alternance\n","\n","\n"],"metadata":{"id":"ahV1fxQ_MOAc"}},{"cell_type":"code","source":["# CDD\n","\n","import io\n","import pandas as pd\n","import requests\n","import re\n","from tqdm import tqdm\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","def api_helloworld_cdd(periode=\"h\", local=\"ile de france\", keyword=\"data\", contrat=\"CDD\"):\n","    results = []\n","\n","    params = {\n","        \"k\": keyword,\n","        \"k_autocomplete\": \"\",\n","        \"l\": local,\n","        \"l_autocomplete\": \"\",\n","        \"sort\": \"relevance\",\n","        \"c\": contrat,\n","        \"cod\": \"all\",\n","        \"ray\": 20,\n","        \"d\": periode\n","    }\n","\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n","        \"Referer\": \"www.hellowork.com\"\n","    }\n","\n","    response = requests.get(\"https://www.hellowork.com/searchoffers/getsearchfacets\", params=params, headers=headers)\n","    response_json = response.json()\n","\n","    if \"Results\" in response_json and response_json[\"Results\"]:\n","        for result in tqdm(response_json[\"Results\"]):\n","            publication_date = result[\"PublishDate\"]\n","            match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', publication_date)\n","            formatted_date = match.group(0) if match else publication_date\n","            lien = result[\"UrlOffre\"]\n","            lien_origine = f'https://www.hellowork.com{lien}'\n","            localisations = result.get(\"Localisations\", [])\n","            ville = localisations[3][\"Label\"] if len(localisations) > 3 else \"N/A\"\n","            result_dict = {\n","                \"entreprise\": result[\"CompanyName\"],\n","                \"publication\": formatted_date,\n","                \"poste\": result[\"OfferTitle\"],\n","                \"contrat\": result[\"ContractType\"],\n","                \"profil\": result[\"Profile\"],\n","                \"description\": result[\"Description\"],\n","                \"ville\": ville,\n","                \"lien\": lien_origine,\n","                \"source\": result[\"ResponseUrl\"]\n","            }\n","            results.append(result_dict)\n","\n","    df = pd.DataFrame(results)\n","    df.reset_index(drop=True, inplace=True)\n","    return df\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Charge les données depuis l'API HelloWork\n","    \"\"\"\n","    df = api_helloworld_cdd(\n","        periode=\"h\",  # h pour aujourd'hui\n","        local=\"ile de france\",\n","        keyword=\"data\",\n","        contrat=\"CDD\"\n","    )\n","    return df\n","\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Test de la sortie du bloc\n","    \"\"\"\n","    assert output is not None, 'La sortie est non définie'\n","    assert isinstance(output, pd.DataFrame), 'La sortie doit être un DataFrame'\n","    assert len(output.columns) > 0, 'Le DataFrame ne doit pas être vide'"],"metadata":{"id":"fCiOQE1fMIZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# alternance\n","\n","import io\n","import pandas as pd\n","import requests\n","import re\n","from tqdm import tqdm\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","def api_helloworld_alternance(periode=\"h\", local=\"ile de france\", keyword=\"data\", contrat=\"Alternance\"):\n","    results = []\n","\n","    params = {\n","        \"k\": keyword,\n","        \"k_autocomplete\": \"\",\n","        \"l\": local,\n","        \"l_autocomplete\": \"\",\n","        \"sort\": \"relevance\",\n","        \"c\": contrat,\n","        \"cod\": \"all\",\n","        \"ray\": 20,\n","        \"d\": periode\n","    }\n","\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n","        \"Referer\": \"www.hellowork.com\"\n","    }\n","\n","    response = requests.get(\"https://www.hellowork.com/searchoffers/getsearchfacets\", params=params, headers=headers)\n","    response_json = response.json()\n","\n","    if \"Results\" in response_json and response_json[\"Results\"]:\n","        for result in tqdm(response_json[\"Results\"]):\n","            publication_date = result[\"PublishDate\"]\n","            match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', publication_date)\n","            formatted_date = match.group(0) if match else publication_date\n","            lien = result[\"UrlOffre\"]\n","            lien_origine = f'https://www.hellowork.com{lien}'\n","            localisations = result.get(\"Localisations\", [])\n","            ville = localisations[3][\"Label\"] if len(localisations) > 3 else \"N/A\"\n","            result_dict = {\n","                \"entreprise\": result[\"CompanyName\"],\n","                \"publication\": formatted_date,\n","                \"poste\": result[\"OfferTitle\"],\n","                \"contrat\": result[\"ContractType\"],\n","                \"profil\": result[\"Profile\"],\n","                \"description\": result[\"Description\"],\n","                \"ville\": ville,\n","                \"lien\": lien_origine,\n","                \"source\": result[\"ResponseUrl\"]\n","            }\n","            results.append(result_dict)\n","\n","    df = pd.DataFrame(results)\n","    df.reset_index(drop=True, inplace=True)\n","    return df\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    df = api_helloworld_alternance(\n","        periode=\"h\",  # h pour aujourd'hui\n","        local=\"ile de france\",\n","        keyword=\"data\",\n","        contrat=\"Alternance\"\n","    )\n","    return df\n","\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Test de la sortie du bloc\n","    \"\"\"\n","    assert output is not None, 'La sortie est non définie'\n","    assert isinstance(output, pd.DataFrame), 'La sortie doit être un DataFrame'\n","    assert len(output.columns) > 0, 'Le DataFrame ne doit pas être vide'"],"metadata":{"id":"PLXzBNOcNDGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# stage\n","\n","import io\n","import pandas as pd\n","import requests\n","import re\n","from tqdm import tqdm\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","def api_helloworld_stage(periode=\"h\", local=\"ile de france\", keyword=\"data\", contrat=\"Stage\"):\n","    results = []\n","\n","    params = {\n","        \"k\": keyword,\n","        \"k_autocomplete\": \"\",\n","        \"l\": local,\n","        \"l_autocomplete\": \"\",\n","        \"sort\": \"relevance\",\n","        \"c\": contrat,\n","        \"cod\": \"all\",\n","        \"ray\": 20,\n","        \"d\": periode\n","    }\n","\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n","        \"Referer\": \"www.hellowork.com\"\n","    }\n","\n","    response = requests.get(\"https://www.hellowork.com/searchoffers/getsearchfacets\", params=params, headers=headers)\n","    response_json = response.json()\n","\n","    if \"Results\" in response_json and response_json[\"Results\"]:\n","        for result in tqdm(response_json[\"Results\"]):\n","            publication_date = result[\"PublishDate\"]\n","            match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', publication_date)\n","            formatted_date = match.group(0) if match else publication_date\n","            lien = result[\"UrlOffre\"]\n","            lien_origine = f'https://www.hellowork.com{lien}'\n","            localisations = result.get(\"Localisations\", [])\n","            ville = localisations[3][\"Label\"] if len(localisations) > 3 else \"N/A\"\n","            result_dict = {\n","                \"entreprise\": result[\"CompanyName\"],\n","                \"publication\": formatted_date,\n","                \"poste\": result[\"OfferTitle\"],\n","                \"contrat\": result[\"ContractType\"],\n","                \"profil\": result[\"Profile\"],\n","                \"description\": result[\"Description\"],\n","                \"ville\": ville,\n","                \"lien\": lien_origine,\n","                \"source\": result[\"ResponseUrl\"]\n","            }\n","            results.append(result_dict)\n","\n","    df = pd.DataFrame(results)\n","    df.reset_index(drop=True, inplace=True)\n","    return df\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    df = api_helloworld_stage(\n","        periode=\"h\",  # h pour aujourd'hui\n","        local=\"ile de france\",\n","        keyword=\"data\",\n","        contrat=\"Stage\"\n","    )\n","    return df\n","\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Test de la sortie du bloc\n","    \"\"\"\n","    assert output is not None, 'La sortie est non définie'\n","    assert isinstance(output, pd.DataFrame), 'La sortie doit être un DataFrame'\n","    assert len(output.columns) > 0, 'Le DataFrame ne doit pas être vide'"],"metadata":{"id":"Bvbj28ECNJNC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*   transformer  \n","qui va fusionner les 3 dataframe en un seul\n","\n"],"metadata":{"id":"S67vV1UZNS_8"}},{"cell_type":"code","source":["import pandas as pd\n","if 'transformer' not in globals():\n","    from mage_ai.data_preparation.decorators import transformer\n","\n","@transformer\n","def merge_dataframes(df_cdd, df_cdi, df_stage, *args, **kwargs):\n","    \"\"\"\n","    Fusionne les trois DataFrames\n","    \"\"\"\n","    df_combined = pd.concat([df_cdd, df_cdi, df_stage], ignore_index=True)\n","\n","    return df_combined\n"],"metadata":{"id":"IAgbaYHINXtv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* transformer 2  \n","pour le nettoyage et la normalisation des données"],"metadata":{"id":"X9OAnGP_N1_1"}},{"cell_type":"code","source":["import pandas as pd\n","if 'transformer' not in globals():\n","    from mage_ai.data_preparation.decorators import transformer\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@transformer\n","def clean_dataframe(df, *args, **kwargs):\n","    \"\"\"\n","    Nettoie le DataFrame\n","    \"\"\"\n","    df_clean = df.copy()\n","    df_clean['publication'] = pd.to_datetime(df_clean['publication'], dayfirst=True, errors='coerce')\n","    df_clean['publication'] = df_clean['publication'].dt.strftime('%d-%m-%Y')\n","    df_clean = df_clean.drop_duplicates()\n","    return df_clean\n","\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Tests de validation\n","    \"\"\"\n","    assert output is not None, 'La sortie est non définie'\n","    assert len(output) > 0, 'Le DataFrame est vide'\n","    date_format = output['publication'].str.match(r'\\d{2}-\\d{2}-\\d{4}')\n","    assert date_format.all(), 'Format de date incorrect'\n","    assert len(output) == len(output.drop_duplicates()), 'Il reste des doublons dans le DataFrame'"],"metadata":{"id":"uGAfGgYON4nu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* exporter  \n","qui va stocker les données sur le bucket minio en chargant le fichier deja existant si il y\"en a un, verifier les doublons et concat le tout."],"metadata":{"id":"ZHCiiKcdNkKE"}},{"cell_type":"code","source":["import pandas as pd\n","from io import BytesIO\n","from minio import Minio\n","import os\n","from datetime import datetime\n","import pytz\n","\n","@data_exporter\n","def export_to_minio(df, *args, **kwargs):\n","    \"\"\"\n","    Exporte le DataFrame des offres vers MinIO en écrasant l'ancien fichier\n","    \"\"\"\n","    client = Minio(\n","        \"minio:9000\",\n","        access_key=os.environ.get('MINIO_ACCESS_KEY'),\n","        secret_key=os.environ.get('MINIO_SECRET_KEY'),\n","        secure=False\n","    )\n","\n","    bucket_name = \"offresdata\"\n","    file_name = \"hellowork.csv\"\n","\n","    if not client.bucket_exists(bucket_name):\n","        client.make_bucket(bucket_name)\n","    df = df.drop_duplicates(subset=['lien'], keep='last')\n","    csv_bytes = df.to_csv(index=False).encode('utf-8')\n","    client.put_object(\n","        bucket_name,\n","        file_name,\n","        BytesIO(csv_bytes),\n","        length=len(csv_bytes),\n","        content_type='application/csv'\n","    )\n","\n","    return f\"{len(df)} offres exportées\"\n","\n","@test\n","def test_output(output, *args) -> None:\n","    assert output is not None and isinstance(output, str), 'La sortie doit être une chaîne non vide'"],"metadata":{"id":"eUShtMHZuzdZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Concatener l'ensembles des CSV du bucket minio :"],"metadata":{"id":"TqOMZ27ZOoZj"}},{"cell_type":"markdown","source":["* charger le csv hellowork :"],"metadata":{"id":"cYKgYLRhPQn-"}},{"cell_type":"code","source":["if 'data_loader':\n","    import pandas as pd\n","    from minio import Minio\n","    import io\n","    import os\n","\n","    @data_loader\n","    def load_hellwork_from_minio(*args, **kwargs):\n","        \"\"\"\n","        Charge le fichier CSV Hellowork depuis Minio\n","        \"\"\"\n","        try:\n","            client = Minio(\n","                \"minio:9000\",\n","                access_key=os.environ.get('MINIO_ACCESS_KEY'),\n","                secret_key=os.environ.get('MINIO_SECRET_KEY'),\n","                secure=False\n","            )\n","            data = client.get_object(\"offresdata\", \"hellowork.csv\")\n","            df = pd.read_csv(io.BytesIO(data.read()))\n","            return df\n","\n","        except Exception as e:\n","            raise e"],"metadata":{"id":"yyJTPMFZPUVo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* charger le csv WTTJ :"],"metadata":{"id":"icFcAGnfPaBa"}},{"cell_type":"code","source":["if 'data_loader':\n","    import pandas as pd\n","    from minio import Minio\n","    import io\n","    import os\n","\n","    @data_loader\n","    def load_wttj_from_minio(*args, **kwargs):\n","        \"\"\"\n","        Charge le fichier CSV Welcome to the Jungle depuis Minio\n","        \"\"\"\n","        try:\n","            client = Minio(\n","                \"minio:9000\",\n","                access_key=os.environ.get('MINIO_ACCESS_KEY'),\n","                secret_key=os.environ.get('MINIO_SECRET_KEY'),\n","                secure=False\n","            )\n","\n","            data = client.get_object(\"offresdata\", \"welcometothejungle.csv\")\n","            df = pd.read_csv(io.BytesIO(data.read()))\n","            return df\n","\n","        except Exception as e:\n","            raise e"],"metadata":{"id":"YgKFIRtLPf2p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* concatenner les CSV :"],"metadata":{"id":"kINBvFWdPxXF"}},{"cell_type":"code","source":["if 'transformer' not in globals():\n","    from mage_ai.data_preparation.decorators import transformer\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@transformer\n","def transform(df_hellowork, df_wttj, *args, **kwargs):\n","    \"\"\"\n","    Fusionne les DataFrames avec les données existantes et sauvegarde dans Minio\n","    \"\"\"\n","    from minio import Minio\n","    import pandas as pd\n","    import io\n","    import os\n","\n","    client = Minio(\n","        \"minio:9000\",\n","        access_key=os.environ.get('MINIO_ACCESS_KEY'),\n","        secret_key=os.environ.get('MINIO_SECRET_KEY'),\n","        secure=False\n","    )\n","\n","    bucket_name = \"offresdata\"\n","    file_name = \"rag_data.csv\"\n","\n","    df_hellowork = pd.DataFrame(df_hellowork) if isinstance(df_hellowork, list) else df_hellowork\n","    df_wttj = pd.DataFrame(df_wttj) if isinstance(df_wttj, list) else df_wttj\n","    df_new = pd.concat([df_hellowork, df_wttj], ignore_index=True)\n","    if not client.bucket_exists(bucket_name):\n","        client.make_bucket(bucket_name)\n","    df_merged = df_new\n","    if client.bucket_exists(bucket_name):\n","        objects = client.list_objects(bucket_name, prefix=file_name)\n","        if any(True for _ in objects):\n","            data = client.get_object(bucket_name, file_name)\n","            df_existing = pd.read_csv(io.BytesIO(data.read()))\n","            df_merged = pd.concat([df_existing, df_new], ignore_index=True)\n","            df_merged = df_merged.drop_duplicates()\n","    csv_buffer = io.BytesIO()\n","    df_merged.to_csv(csv_buffer, index=False, encoding='utf-8')\n","    csv_buffer.seek(0)\n","    client.put_object(\n","        bucket_name,\n","        file_name,\n","        csv_buffer,\n","        length=csv_buffer.getbuffer().nbytes,\n","        content_type='text/csv'\n","    )\n","    print(df_merged.info())\n","    return df_merged\n","\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"],"metadata":{"id":"DuoUOQRiP1Ku"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* exporter les données vers le bucket minio :  \n","les données sont sauvegardé au format parquet."],"metadata":{"id":"X1hcpj35Qcbv"}},{"cell_type":"code","source":["import pandas as pd\n","from io import BytesIO\n","from minio import Minio\n","import os\n","from datetime import datetime\n","import pytz\n","\n","@data_exporter\n","def export_to_minio(df, *args, **kwargs):\n","    \"\"\"\n","    Exporte le DataFrame des offres vers MinIO en écrasant l'ancien fichier\n","    \"\"\"\n","    client = Minio(\n","        \"minio:9000\",\n","        access_key=os.environ.get('MINIO_ACCESS_KEY'),\n","        secret_key=os.environ.get('MINIO_SECRET_KEY'),\n","        secure=False\n","    )\n","\n","    bucket_name = \"offresdata\"\n","    file_name = \"rag_data.csv\"\n","\n","    if not client.bucket_exists(bucket_name):\n","        client.make_bucket(bucket_name)\n","    df = df.drop_duplicates(subset=['lien'], keep='last')\n","    csv_bytes = df.to_csv(index=False).encode('utf-8')\n","    client.put_object(\n","        bucket_name,\n","        file_name,\n","        BytesIO(csv_bytes),\n","        length=len(csv_bytes),\n","        content_type='application/csv'\n","    )\n","\n","    return f\"{len(df)} offres exportées\"\n","\n","@test\n","def test_output(output, *args) -> None:\n","    assert output is not None and isinstance(output, str), 'La sortie doit être une chaîne non vide'\n","\n","\n"],"metadata":{"id":"-mi6B7yYQgMS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# nettoyage"],"metadata":{"id":"wpepMuJf1uVE"}},{"cell_type":"code","source":["if 'data_loader':\n","    import pandas as pd\n","    from minio import Minio\n","    import io\n","    import os\n","\n","    @data_loader\n","    def load_hellwork_from_minio(*args, **kwargs):\n","        try:\n","            client = Minio(\n","                \"minio:9000\",\n","                access_key=os.environ.get('MINIO_ACCESS_KEY'),\n","                secret_key=os.environ.get('MINIO_SECRET_KEY'),\n","                secure=False\n","            )\n","            data = client.get_object(\"offresdata\", \"rag_data.csv\")\n","            df = pd.read_csv(io.BytesIO(data.read()))\n","            return df\n","\n","        except Exception as e:\n","            raise e"],"metadata":{"id":"ngUtuYpX1v2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'transformer' not in globals():\n","    from mage_ai.data_preparation.decorators import transformer\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","import pandas as pd\n","import re\n","\n","@transformer\n","def transform_clean_poste(data, *args, **kwargs):\n","    def clean_job_title(title):\n","        if not isinstance(title, str):\n","            return \"Poste Non Spécifié\"\n","        cleaned = title.lower()\n","\n","        patterns = [\n","            r\"\\b(stage|alternance|cdd|cdi|alternant|stagiaire|intern|intérim)\\b\",\n","            r\"\\b(bac\\s*\\+\\s*\\d+|fin\\s+d['\\']études|étudiant|formation|école|master|licence|mastère)\\b\",\n","            r\"\\b(paris|grenoble|lyon|france|île-de-france|idf)\\b\",\n","            r\"\\b(h/f|f/h|h-f|f-h|m/f/d)\\b\",\n","            r\"[-–—\\s]*[(]?(h/f|f/h|h-f|f-h|m/f/d)[)]?\",\n","            r\"\\b(temps\\s+plein|temps\\s+partiel|débutant|confirmé|junior|senior)\\b\",\n","            r\"\\b(\\d+\\s*(mois|ans?|années?|months?|years?))\\b\",\n","            r\"\\b(deepki|ref\\s*:\\s*[^\\s]+)\\b\",\n","            r\"\\b(ref|reference)\\s*\\.?\\s*:?\\s*[a-z0-9]+\\b\",\n","            r\"^(de|en|pour|le|la|les|un|une|des)\\s+\",\n","            r\"\\s+(de|en|pour|le|la|les|un|une|des)$\"\n","        ]\n","\n","        cleaned = re.sub(r'\\([^)]*\\)', '', cleaned)\n","        for pattern in patterns:\n","            cleaned = re.sub(pattern, ' ', cleaned, flags=re.IGNORECASE)\n","        cleaned = re.sub(r'[,;:]', ' ', cleaned)\n","        cleaned = re.sub(r'[-–—]{2,}', ' ', cleaned)\n","        cleaned = re.sub(r'^[-–—]+|[-–—]+$', '', cleaned)\n","        cleaned = re.sub(r'\\s+[-–—]+\\s+', ' ', cleaned)\n","        cleaned = re.sub(r'\\s+[-–—]\\s+', ' ', cleaned)\n","        cleaned = re.sub(r'\\s*[-–—]\\s*$', '', cleaned)\n","        cleaned = re.sub(r'^\\s*[-–—]\\s*', '', cleaned)\n","        cleaned = re.sub(r'\\s+', ' ', cleaned)\n","        cleaned = cleaned.strip()\n","        cleaned = ' '.join(word.capitalize() for word in cleaned.split())\n","        cleaned = re.sub(r'\\s*[-–—]+$', '', cleaned)\n","        cleaned = re.sub(r'\\d+', '', cleaned)\n","\n","        return cleaned\n","\n","    data['poste'] = data['poste'].apply(clean_job_title)\n","\n","    return data\n","\n","@test\n","def test_output(output, *args) -> None:\n","    assert output is not None, 'Le output ne peut pas être None'\n","    assert 'poste' in output.columns, 'La colonne poste doit être présente'\n","    assert output['poste'].isnull().sum() == 0, 'Il ne doit pas y avoir de valeurs nulles'"],"metadata":{"id":"mM9UpGUP1zNO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'transformer' not in globals():\n","    from mage_ai.data_preparation.decorators import transformer\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","import pandas as pd\n","\n","@transformer\n","def transform_clean_contrat(data, *args, **kwargs):\n","    data['contrat'] = data['contrat'].astype(str).str.replace(r'\\s*\\(.*\\)', '', regex=True).str.strip()\n","    data['contrat'] = data['contrat'].replace('CDD / Temporaire', 'CDD')\n","\n","    return data\n","\n","@test\n","def test_output(output, *args) -> None:\n","    assert output is not None, 'Le output ne peut pas être None'\n","    assert 'contrat' in output.columns, 'La colonne contrat doit être présente'\n","    assert not output['contrat'].str.contains(r'\\(.*\\)').any(), 'Il reste des parenthèses dans les contrats'\n","    assert not (output['contrat'] == 'CDD / Temporaire').any(), 'Il reste des valeurs non remplacées'"],"metadata":{"id":"5go8304213q4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'transformer' not in globals():\n","    from mage_ai.data_preparation.decorators import transformer\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","import pandas as pd\n","\n","@transformer\n","def transform_merge_description(data, *args, **kwargs):\n","    \"\"\"\n","    Fusionne les colonnes description et profil, supprime les colonnes non nécessaires,\n","    et génère des statistiques sur le dataset.\n","\n","    Args:\n","        data: Le DataFrame avec les colonnes à fusionner\n","\n","    Returns:\n","        DataFrame: Le DataFrame avec les colonnes fusionnées et nettoyé\n","    \"\"\"\n","    # Fusion des colonnes\n","    data['description_poste'] = (data['description'].fillna('').astype(str) +\n","                                '\\n\\n' +\n","                                data['profil'].fillna('').astype(str))\n","\n","    # Nettoyage des cas où les deux colonnes étaient vides\n","    data['description_poste'] = data['description_poste'].replace('^\\n\\n$', '', regex=True)\n","\n","    # Suppression des colonnes originales et de la colonne source\n","    data = data.drop(['description', 'profil', 'source'], axis=1)\n","\n","    # Génération et affichage des statistiques\n","    print(\"\\n=== Statistiques du Dataset ===\\n\")\n","\n","    # Nombre total de lignes et colonnes\n","    print(f\"Dimensions du DataFrame : {data.shape[0]} lignes × {data.shape[1]} colonnes\\n\")\n","\n","    # Statistiques pour chaque colonne\n","    for column in data.columns:\n","        print(f\"\\n=== Statistiques pour la colonne '{column}' ===\")\n","\n","        # Nombre de valeurs manquantes\n","        missing_count = data[column].isna().sum()\n","        missing_percentage = (missing_count / len(data)) * 100\n","        print(f\"Valeurs manquantes : {missing_count} ({missing_percentage:.2f}%)\")\n","\n","        # Nombre de valeurs uniques\n","        unique_count = data[column].nunique()\n","        print(f\"Valeurs uniques : {unique_count}\")\n","\n","        # Affichage des valeurs uniques pour les colonnes catégorielles\n","        if data[column].dtype == 'object' and unique_count < 20:  # Limite à 20 valeurs uniques\n","            print(\"Valeurs uniques et leurs fréquences :\")\n","            value_counts = data[column].value_counts().head(10)  # Top 10 des valeurs les plus fréquentes\n","            for value, count in value_counts.items():\n","                percentage = (count / len(data)) * 100\n","                print(f\"  - {value}: {count} occurrences ({percentage:.2f}%)\")\n","\n","        print(\"\\n\" + \"=\"*50)\n","\n","    return data\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Test la validité du DataFrame de sortie.\n","    \"\"\"\n","    assert output is not None, 'Le output ne peut pas être None'\n","    assert 'description_poste' in output.columns, 'La colonne description_poste doit être présente'\n","    assert 'description' not in output.columns, 'La colonne description doit être supprimée'\n","    assert 'profil' not in output.columns, 'La colonne profil doit être supprimée'\n","    assert 'source' not in output.columns, 'La colonne source doit être supprimée'\n","    # Vérifie qu'il n'y a pas que des \\n\\n dans la colonne description_poste\n","    assert not (output['description_poste'] == '\\n\\n').any(), 'Il reste des lignes vides non nettoyées'"],"metadata":{"id":"U6zEaJaQ15of"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from io import BytesIO\n","from minio import Minio\n","import os\n","from datetime import datetime\n","import pytz\n","\n","@data_exporter\n","def export_to_minio(df, *args, **kwargs):\n","    \"\"\"\n","    Exporte le DataFrame des offres vers MinIO en écrasant l'ancien fichier\n","    \"\"\"\n","    client = Minio(\n","        \"minio:9000\",\n","        access_key=os.environ.get('MINIO_ACCESS_KEY'),\n","        secret_key=os.environ.get('MINIO_SECRET_KEY'),\n","        secure=False\n","    )\n","\n","    bucket_name = \"offresdata\"\n","    file_name = \"rag_data_clean.csv\"\n","\n","    if not client.bucket_exists(bucket_name):\n","        client.make_bucket(bucket_name)\n","    df = df.drop_duplicates(subset=['lien'], keep='last')\n","    csv_bytes = df.to_csv(index=False).encode('utf-8')\n","    client.put_object(\n","        bucket_name,\n","        file_name,\n","        BytesIO(csv_bytes),\n","        length=len(csv_bytes),\n","        content_type='application/csv'\n","    )\n","\n","    return f\"{len(df)} offres exportées\"\n","\n","@test\n","def test_output(output, *args) -> None:\n","    assert output is not None and isinstance(output, str), 'La sortie doit être une chaîne non vide'"],"metadata":{"id":"86FpI1s519ey"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# export vers postgresML"],"metadata":{"id":"Xmufg-P38rfR"}},{"cell_type":"code","source":["if 'data_loader':\n","    import pandas as pd\n","    from minio import Minio\n","    import io\n","    import os\n","\n","    @data_loader\n","    def load_hellwork_from_minio(*args, **kwargs):\n","        \"\"\"\n","        Charge le fichier CSV Hellwork depuis Minio\n","        \"\"\"\n","        try:\n","            client = Minio(\n","                \"minio:9000\",\n","                access_key=os.environ.get('MINIO_ACCESS_KEY'),\n","                secret_key=os.environ.get('MINIO_SECRET_KEY'),\n","                secure=False\n","            )\n","            data = client.get_object(\"offresdata\", \"rag_data_clean.csv\")\n","            df = pd.read_csv(io.BytesIO(data.read()))\n","            print(df.dtypes)\n","            return df\n","\n","        except Exception as e:\n","            raise e"],"metadata":{"id":"ydeYqzLj8uXt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'transformer' not in globals():\n","    from mage_ai.data_preparation.decorators import transformer\n","    from mage_ai.data_preparation.shared.secrets import get_secret_value\n","import pandas as pd\n","from datetime import date\n","\n","@transformer\n","def transform_data(df, *args, **kwargs):\n","    \"\"\"\n","    Prépare les données pour l'insertion dans une base SQL\n","    \"\"\"\n","    df_clean = df.copy()\n","    df_clean['publication'] = pd.to_datetime(df_clean['publication'], format='%d-%m-%Y').dt.strftime('%d/%m/%Y')\n","    print(df_clean.dtypes)\n","    return df_clean"],"metadata":{"id":"BzZ0roNY8wWz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["considerer le temps d'execution de l'embedding dans la logique du pipeline (plus ou moins 20 minutes)"],"metadata":{"id":"MA8PBdDDVPS2"}},{"cell_type":"code","source":["import os\n","import psycopg2\n","from psycopg2.extras import execute_values\n","import pandas as pd\n","\n","if 'data_exporter' not in globals():\n","    from mage_ai.data_preparation.decorators import data_exporter\n","\n","@data_exporter\n","def export_data(df, **kwargs):\n","    \"\"\"\n","    Exporte les données vers PostgreSQL et gère les doublons\n","    \"\"\"\n","    try:\n","        # Connexion à la base de données\n","        connection = psycopg2.connect(\n","            host=\"unruffled_visvesvaraya\",  # Nom du service/conteneur PostgreSQL dans docker\n","            port=5432,  # Le port de connection au container\n","            database=\"projet_fil_rouge\",  # Le nom de la bdd\n","            user=\"postgres\",\n","            password=\"postgres\"\n","        )\n","\n","        with connection.cursor() as cur:\n","            print(\"Connexion à la base de données\")\n","\n","            # Vérifier les doublons avant l'insertion des données\n","            print(\"Vérification des doublons\")\n","            rows_to_drop = []\n","            for index, row in df.iterrows():\n","                cur.execute(\"\"\"\n","                    SELECT COUNT(*)\n","                    FROM data_fil_rouge\n","                    WHERE entreprise = %s\n","                    AND publication = %s\n","                    AND poste = %s\n","                    AND contrat = %s\n","                    AND ville = %s\n","                    AND lien = %s\n","                    AND description_poste = %s;\n","                \"\"\", (row[\"entreprise\"], row[\"publication\"], row[\"poste\"], row[\"contrat\"], row[\"ville\"], row[\"lien\"], row[\"description_poste\"]))\n","                if cur.fetchone()[0] > 0:\n","                    print(f\"Doublon trouvé et supprimé : {row}\")\n","                    rows_to_drop.append(index)\n","\n","            # Supprimer les doublons en une seule opération\n","            if rows_to_drop:\n","                df = df.drop(rows_to_drop)\n","\n","            # Préparer les données\n","            print(\"Préparation des données\")\n","            tuples = [tuple(x) for x in df.to_numpy()]\n","            cols = ','.join(list(df.columns))\n","            query = f\"INSERT INTO data_fil_rouge ({cols}) VALUES %s\"\n","\n","            # Insérer les nouvelles données\n","            if len(tuples) > 0:\n","                print(\"Insertion des nouvelles données\")\n","                execute_values(cur, query, tuples)\n","\n","                # Générer les embeddings pour les nouvelles descriptions de poste\n","                print(\"Génération des embeddings\")\n","                cur.execute(\"\"\"\n","                    UPDATE data_fil_rouge\n","                    SET embedding = pgml.embed('intfloat/e5-large', description_poste)\n","                    WHERE embedding IS NULL;\n","                \"\"\")\n","\n","                connection.commit()\n","                print(\"Nouvelles données insérées!\")\n","            else:\n","                print(\"Aucune nouvelle donnée à insérer.\")\n","\n","        # Fermer la connexion\n","        connection.close()\n","\n","    except psycopg2.Error as e:\n","        print(f\"Erreur PostgreSQL: {e}\")\n","    except Exception as e:\n","        print(f\"Erreur : {e}\")"],"metadata":{"id":"ae6li1C58yp8"},"execution_count":null,"outputs":[]}]}